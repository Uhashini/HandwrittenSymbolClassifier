{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEIhSerw+v3euV+nZw361n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uhashini/HandwrittenSymbolClassifier/blob/main/HandwrittenSymbolClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "BMuOj8xSvS2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AEZ-r--3P-Gq",
        "outputId": "bac0b56d-37ef-49dc-db9b-d0503e608b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to: /content/dataset\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "zip_path = \"/content/torch-it-up.zip\"  # Replace with your uploaded file path\n",
        "extract_path = \"/content/dataset\"\n",
        "\n",
        "# Extract the zip file\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Dataset extracted to: {extract_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load CSV Files and Verify Data"
      ],
      "metadata": {
        "id": "-Er7GgoWvvaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define correct paths\n",
        "train_csv_path = \"/content/dataset/dataset/train.csv\"\n",
        "test_csv_path = \"/content/dataset/dataset/test.csv\"\n",
        "image_folder = \"/content/dataset/dataset/Dataset_Image/Dataset_Image/data/\"\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Correct the image paths\n",
        "train_df[\"image_path\"] = train_df[\"image_path\"].apply(lambda x: os.path.join(image_folder, os.path.basename(x)))\n",
        "test_df[\"image_path\"] = test_df[\"image_path\"].apply(lambda x: os.path.join(image_folder, os.path.basename(x)))\n",
        "\n",
        "# Verify a sample image\n",
        "sample_image = train_df.iloc[0, 0]\n",
        "print(\"Sample image path:\", sample_image)\n",
        "print(\"Does this file exist?\", os.path.exists(sample_image))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "avS4AVaeR1i3",
        "outputId": "b4a1c3a7-28f8-43fa-915c-33782dbbb137"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image path: /content/dataset/dataset/Dataset_Image/Dataset_Image/data/v2-118246.png\n",
            "Does this file exist? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Preprocessing & DataLoader Setup"
      ],
      "metadata": {
        "id": "RH6Z6fMsv9EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize for better training\n",
        "])\n",
        "\n",
        "# Custom PyTorch Dataset class\n",
        "class MathSymbolsDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None, train=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.iloc[idx, 0]\n",
        "        image = Image.open(img_name).convert('L')  # Open image\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.train:\n",
        "            label = int(self.dataframe.iloc[idx, 1])  # Get label for training\n",
        "            return image, label\n",
        "        else:\n",
        "            example_id = int(self.dataframe.iloc[idx, 2])  # Test set (no labels)\n",
        "            return image, example_id\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = MathSymbolsDataset(train_df, transform=transform, train=True)\n",
        "test_dataset = MathSymbolsDataset(test_df, transform=transform, train=False)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"‚úÖ Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"‚úÖ Test dataset size: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rWUYn840VIOn",
        "outputId": "70b6f9a3-4d9d-4364-b364-f8a8eebda3ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Train dataset size: 134586\n",
            "‚úÖ Test dataset size: 33647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the CNN Model"
      ],
      "metadata": {
        "id": "_jLkfrUkwCUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define CNN model\n",
        "class SymbolClassifierCNN(nn.Module):\n",
        "    def __init__(self, num_classes=369):\n",
        "        super(SymbolClassifierCNN, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),  # Flattened 32x32 image\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten before FC layers\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SymbolClassifierCNN().to(device)\n",
        "\n",
        "print(model)  # Print model summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IHA6pJUtVMeA",
        "outputId": "78c5aa0d-3086-4909-879d-ef880efd2819"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SymbolClassifierCNN(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layers): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=369, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max label:\", train_df[\"label\"].max())\n",
        "print(\"Min label:\", train_df[\"label\"].min())\n",
        "print(\"Unique labels:\", train_df[\"label\"].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oZ8snSnzVmTg",
        "outputId": "6ef4959c-8086-45d6-d72d-341388c9b4a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max label: 1400\n",
            "Min label: 31\n",
            "Unique labels: 369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in dataset:\", sorted(train_df[\"label\"].unique())[:20])  # Show first 20 labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "71Z0cv05Vzsn",
        "outputId": "97ceb6f5-7096-4dad-ef8c-2adcec0cb12a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in dataset: [31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift labels so that they start from 0 instead of 31\n",
        "train_df[\"label\"] = train_df[\"label\"] - 31\n",
        "\n",
        "# Verify the fix\n",
        "print(\"Updated Unique Labels:\", sorted(train_df[\"label\"].unique()))\n",
        "print(\"Max label after fix:\", train_df[\"label\"].max())  # Should be 368\n",
        "print(\"Min label after fix:\", train_df[\"label\"].min())  # Should be 0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kllR3s99V7bd",
        "outputId": "098753a2-f0a8-4e2b-f520-f172af4a7f74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Unique Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 223, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 477, 479, 480, 481, 482, 483, 486, 489, 490, 492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 513, 518, 519, 522, 524, 531, 533, 543, 546, 551, 552, 553, 560, 564, 569, 570, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 589, 590, 591, 599, 600, 603, 604, 605, 608, 609, 613, 616, 619, 630, 640, 647, 648, 652, 653, 667, 680, 681, 682, 685, 697, 708, 710, 712, 717, 720, 722, 725, 726, 727, 728, 730, 731, 732, 733, 734, 736, 737, 739, 740, 744, 746, 747, 752, 754, 755, 757, 760, 761, 770, 778, 781, 786, 791, 792, 796, 806, 807, 850, 851, 853, 854, 855, 856, 857, 858, 859, 860, 861, 863, 870, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 903, 905, 910, 912, 913, 914, 915, 916, 917, 918, 919, 920, 922, 925, 926, 927, 928, 929, 934, 937, 940, 941, 942, 943, 946, 961, 962, 963, 964, 965, 966, 967, 968, 969, 973, 974, 975, 976, 977, 979, 980, 981, 982, 985, 987, 988, 1000, 1006, 1011, 1014, 1015, 1020, 1022, 1031, 1033, 1034, 1035, 1043, 1044, 1046, 1047, 1048, 1049, 1051, 1055, 1059, 1062, 1070, 1071, 1072, 1080, 1081, 1084, 1085, 1086, 1137, 1138, 1146, 1153, 1154, 1156, 1283, 1284, 1285, 1286, 1338, 1340, 1343, 1351, 1354, 1363, 1364, 1365, 1369]\n",
            "Max label after fix: 1369\n",
            "Min label after fix: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get unique labels sorted\n",
        "unique_labels = sorted(train_df[\"label\"].unique())\n",
        "\n",
        "# Create a mapping dictionary (old label ‚Üí new index)\n",
        "label_map = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
        "\n",
        "# Apply mapping to train dataset\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# Verify fixed labels\n",
        "print(\"Updated Unique Labels:\", sorted(train_df[\"label\"].unique()))\n",
        "print(\"Max label after fix:\", train_df[\"label\"].max())  # Should be 368\n",
        "print(\"Min label after fix:\", train_df[\"label\"].min())  # Should be 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-grmmFvZWFyQ",
        "outputId": "7709a8eb-f1fe-404f-8ba6-030776259e3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Unique Labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368]\n",
            "Max label after fix: 368\n",
            "Min label after fix: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "fNbomdWwwS9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset with fixed labels\n",
        "train_dataset = MathSymbolsDataset(train_df, transform=transform, train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Increase for better performance\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete! üéâ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Zfbln2T8WOn3",
        "outputId": "c45ad764-6e78-4892-bd2a-4460f4686afb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7324\n",
            "Epoch [2/10], Loss: 0.6672\n",
            "Epoch [3/10], Loss: 0.6256\n",
            "Epoch [4/10], Loss: 0.5939\n",
            "Epoch [5/10], Loss: 0.5561\n",
            "Epoch [6/10], Loss: 0.5306\n",
            "Epoch [7/10], Loss: 0.5116\n",
            "Epoch [8/10], Loss: 0.4870\n",
            "Epoch [9/10], Loss: 0.4678\n",
            "Epoch [10/10], Loss: 0.4477\n",
            "‚úÖ Training complete! üéâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Inference on the Test Set\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CGsQbEGMwtDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Store predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, example_ids in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted_labels = torch.max(outputs, 1)  # Get highest probability class\n",
        "\n",
        "        # Store predictions\n",
        "        for example_id, label in zip(example_ids, predicted_labels.cpu().numpy()):\n",
        "            predictions.append((example_id, label))\n",
        "\n",
        "# Convert to DataFrame\n",
        "submission_df = pd.DataFrame(predictions, columns=[\"example_id\", \"label\"])\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to submission.csv!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MUBHpzzsqPR2",
        "outputId": "f8a5d30f-a47e-443f-f661-a7900bd355cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View First Few Predictions"
      ],
      "metadata": {
        "id": "Fp_Wyh_9xF-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the submission file\n",
        "submission_df = pd.read_csv(\"submission.csv\")\n",
        "\n",
        "# Display first 10 rows\n",
        "print(submission_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0V8eK4byq_UP",
        "outputId": "4fd1e03c-eba9-44b2-ecf0-5320005e2e70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       example_id  label\n",
            "0  tensor(114921)    227\n",
            "1  tensor(121504)    146\n",
            "2   tensor(74325)    131\n",
            "3   tensor(30262)    270\n",
            "4  tensor(109694)    211\n",
            "5  tensor(153893)     73\n",
            "6  tensor(102807)    189\n",
            "7   tensor(23294)     38\n",
            "8  tensor(123049)    250\n",
            "9   tensor(94755)    172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Training Data into Train & Validation Sets"
      ],
      "metadata": {
        "id": "TzejLiDpxYui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split train data (80% training, 20% validation)\n",
        "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label\"])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MathSymbolsDataset(train_data, transform=transform, train=True)\n",
        "val_dataset = MathSymbolsDataset(val_data, transform=transform, train=True)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"‚úÖ Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R4tiTMbPrOCQ",
        "outputId": "e38ffb56-2484-472a-a2de-21dc4f4b0583"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training samples: 107668, Validation samples: 26918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oA9qfwAWrcMQ",
        "outputId": "00b7ff22-fb48-4ed0-d9a7-45def0efa0a8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Training Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "OxNQjZPwxblp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Training Accuracy\n",
        "model.eval()\n",
        "correct_train = 0\n",
        "total_train = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "# Compute final training accuracy\n",
        "train_accuracy = correct_train / total_train * 100\n",
        "print(f\" Final Training Accuracy: {train_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MIPgRSmJrTNK",
        "outputId": "7e96d3b3-fd6f-4502-d398-b557ab7a6f0a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final Training Accuracy: 88.70%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Validation Accuracy"
      ],
      "metadata": {
        "id": "5XxnqimPxio_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Validation Accuracy\n",
        "model.eval()\n",
        "correct_val = 0\n",
        "total_val = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        correct_val += (predicted == labels).sum().item()\n",
        "        total_val += labels.size(0)\n",
        "\n",
        "# Compute final validation accuracy\n",
        "val_accuracy = correct_val / total_val * 100\n",
        "print(f\"‚úÖ Final Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9TBBWNVotuHG",
        "outputId": "1f2d9ddb-ebe5-499c-f8a7-a67d6093dd26"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final Validation Accuracy: 88.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict a Single Image & Display It"
      ],
      "metadata": {
        "id": "vL3MDhwFxqoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Function to predict a single image\n",
        "def predict_single_image(image_path, model, class_labels=None):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Define transformation (same as used during training)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
        "    ])\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    image_transformed = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        output = model(image_transformed)\n",
        "        _, predicted_label = torch.max(output, 1)\n",
        "\n",
        "    predicted_label = predicted_label.item()  # Convert tensor to integer\n",
        "\n",
        "    # Display the image and prediction\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"Predicted Label: {predicted_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Return the predicted label\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/dataset/dataset/Dataset_Image/Dataset_Image/data/v2-118246.png\"  # Replace with any image path\n",
        "predicted_class = predict_single_image(image_path, model)\n",
        "\n",
        "print(f\" Predicted Class: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "IeWAGf1LuAjB",
        "outputId": "259ba0ad-c465-45ff-cb38-3796affa0dcd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Predicted Class: 236\n"
          ]
        }
      ]
    }
  ]
}